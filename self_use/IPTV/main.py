import re
import requests
import logging
import warnings
from collections import OrderedDict
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple, Set
import config  # å¯¼å…¥é…ç½®æ–‡ä»¶
import os

# ===================== åŸºç¡€é…ç½® =====================
# å±è”½SSLä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# ç¡®ä¿ output æ–‡ä»¶å¤¹å­˜åœ¨
OUTPUT_FOLDER = Path("output")
OUTPUT_FOLDER.mkdir(exist_ok=True)

# GitHub é•œåƒåŸŸååˆ—è¡¨
GITHUB_MIRRORS = [
    "raw.githubusercontent.com",
    "raw.kkgithub.com",
    "raw.githubusercontents.com",
    "raw.fgit.cf",
    "raw.fgithub.de"
]

# ä»£ç†å‰ç¼€åˆ—è¡¨
PROXY_PREFIXES = [
    "https://ghproxy.com/",
    "https://mirror.ghproxy.com/",
    "https://gh.api.99988866.xyz/"
]

# æ—¥å¿—é…ç½®
LOG_FILE_PATH = OUTPUT_FOLDER / "live_source_extract.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE_PATH, "w", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ===================== æ•°æ®ç»“æ„ =====================
@dataclass
class ChannelMeta:
    """é¢‘é“å…ƒä¿¡æ¯ï¼ˆå®Œæ•´ä¿ç•™åŸå§‹M3Uæ ‡ç­¾ï¼‰"""
    url: str  # å¿…å¡«ï¼šæ’­æ”¾URL
    raw_extinf: str = ""  # å®Œæ•´çš„åŸå§‹#EXTINFè¡Œ
    tvg_id: Optional[str] = None  # åŸå§‹tvg-id
    tvg_name: Optional[str] = None  # åŸå§‹tvg-name
    tvg_logo: Optional[str] = None  # åŸå§‹tvg-logo
    group_title: Optional[str] = None  # æ ‡å‡†åŒ–åçš„group-title
    channel_name: Optional[str] = None  # æ ‡å‡†åŒ–åçš„é¢‘é“å
    source_url: str = ""  # æ¥æºURL

# å…¨å±€å­˜å‚¨
channel_meta_cache: Dict[str, ChannelMeta] = {}  # key: url, value: ChannelMeta
url_source_mapping: Dict[str, str] = {}  # url -> æ¥æºURL

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def clean_group_title(group_title: str) -> str:
    """
    æ ‡å‡†åŒ–group-titleï¼š
    1. ä¼˜å…ˆåŒ¹é…configä¸­çš„åœ°åŒºâ†’é¢‘é“æ˜ å°„
    2. è¿‡æ»¤emojiã€ç‰¹æ®Šç¬¦å·ï¼Œä¿ç•™æ ¸å¿ƒæ–‡å­—
    :param group_title: åŸå§‹group-titleï¼ˆå«emoji/ç‰¹æ®Šç¬¦å·/åœ°åŒºåç§°ï¼‰
    :return: æ ‡å‡†åŒ–åçš„çº¯æ–‡å­—group-title
    """
    if not group_title:
        return "æœªåˆ†ç±»"
    
    # æ­¥éª¤1ï¼šä¼˜å…ˆåŒ¹é…configä¸­çš„group-titleæ˜ å°„ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
    original_title = group_title.strip()
    if original_title in config.group_title_mapping:
        mapped_title = config.group_title_mapping[original_title]
        logger.debug(f"group-titleæ˜ å°„åŒ¹é…ï¼š{original_title} â†’ {mapped_title}")
        group_title = mapped_title
    else:
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†å¸¦emoji/ç‰¹æ®Šç¬¦å·çš„æƒ…å†µï¼Œå¦‚"ğŸ”¥å®‰å¾½åœ°åŒº"ï¼‰
        # æå–çº¯æ–‡å­—éƒ¨åˆ†å†åŒ¹é…æ˜ å°„
        pure_text = ''.join(re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]+', original_title))
        if pure_text in config.group_title_mapping:
            mapped_title = config.group_title_mapping[pure_text]
            logger.debug(f"group-titleæ¨¡ç³Šæ˜ å°„åŒ¹é…ï¼š{original_title} â†’ {mapped_title}")
            group_title = mapped_title
    
    # æ­¥éª¤2ï¼šè¿‡æ»¤emojiã€ç‰¹æ®Šç¬¦å·ï¼Œä¿ç•™æ ¸å¿ƒæ–‡å­—
    cleaned = re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9_\(\)]+', group_title)
    result = ''.join(cleaned).strip() or "æœªåˆ†ç±»"
    
    # æ­¥éª¤3ï¼šé•¿åº¦å…œåº•ï¼ˆè¶…è¿‡20å­—æˆªå–ï¼‰
    if len(result) > 20:
        result = result[:20]
    
    logger.debug(f"group-titleæœ€ç»ˆæ ‡å‡†åŒ–ï¼š{original_title} â†’ {result}")
    return result

def global_replace_cctv_name(content: str) -> str:
    """
    å¯¹æºå†…å®¹åšå…¨å±€å¤®è§†é¢‘é“åç§°æ›¿æ¢ï¼ˆå…ˆé•¿åçŸ­ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
    :param content: æŠ“å–çš„åŸå§‹æºå†…å®¹
    :return: æ›¿æ¢åçš„æ ‡å‡†åŒ–å†…å®¹
    """
    if not content:
        return content
    
    # åˆå¹¶æ˜ å°„å¹¶æŒ‰åç§°é•¿åº¦é™åºæ’åºï¼ˆå…ˆæ›¿æ¢é•¿åç§°ï¼Œé¿å…"CCTV5"å…ˆæ›¿æ¢å¯¼è‡´"CCTV5+"åŒ¹é…å¤±è´¥ï¼‰
    all_mappings = {}
    # å…ˆåŠ å…¥åŸºç¡€æ˜ å°„
    all_mappings.update(config.cntvNamesReverse)
    # å†åŠ å…¥åˆ«åæ˜ å°„ï¼ˆåˆ«åæ˜ å°„å¯è¦†ç›–åŸºç¡€æ˜ å°„ï¼Œè‹¥æœ‰é‡å¤ï¼‰
    all_mappings.update(config.cctv_alias)
    # æŒ‰åç§°é•¿åº¦é™åºã€åç§°å­—æ¯é™åºæ’åºï¼Œç¡®ä¿é•¿åç§°ä¼˜å…ˆæ›¿æ¢
    sorted_mappings = sorted(all_mappings.items(), key=lambda x: (-len(x[0]), x[0]))
    
    # å…¨å±€æ›¿æ¢
    replaced_content = content
    for old_name, new_name in sorted_mappings:
        if old_name in replaced_content:
            replaced_content = replaced_content.replace(old_name, new_name)
            logger.debug(f"å…¨å±€æ›¿æ¢é¢‘é“åï¼š{old_name} â†’ {new_name}")
    
    return replaced_content

def standardize_cctv_name(channel_name: str) -> str:
    """
    æ ‡å‡†åŒ–å•ä¸ªå¤®è§†é¢‘é“åç§°ï¼ˆå…œåº•å¤„ç†ï¼Œé˜²æ­¢å…¨å±€æ›¿æ¢é—æ¼ï¼‰
    :param channel_name: åŸå§‹é¢‘é“åç§°
    :return: æ ‡å‡†åŒ–åçš„åç§°
    """
    if not channel_name:
        return channel_name
    
    # å…ˆåŒ¹é…åŸºç¡€æ˜ å°„
    if channel_name in config.cntvNamesReverse:
        return config.cntvNamesReverse[channel_name]
    # å†åŒ¹é…åˆ«åæ˜ å°„
    if channel_name in config.cctv_alias:
        return config.cctv_alias[channel_name]
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¤„ç†å¸¦å¤šä½™å­—ç¬¦çš„æƒ…å†µï¼‰
    normalized_name = channel_name.strip()
    for raw_name, standard_name in config.cntvNamesReverse.items():
        if raw_name in normalized_name:
            return standard_name
    for alias_name, standard_name in config.cctv_alias.items():
        if alias_name in normalized_name:
            return standard_name
    # æ— åŒ¹é…åˆ™è¿”å›åŸåç§°
    return channel_name

def replace_github_domain(url: str) -> List[str]:
    """æ›¿æ¢GitHubåŸŸåï¼ˆè‡ªåŠ¨ä¿®å¤GitHub URLï¼‰"""
    if not url or "github" not in url.lower():
        return [url]
    
    candidate_urls = [url]
    # æ›¿æ¢é•œåƒåŸŸå
    for mirror in GITHUB_MIRRORS:
        for original in GITHUB_MIRRORS:
            if original in url:
                new_url = url.replace(original, mirror)
                if new_url not in candidate_urls:
                    candidate_urls.append(new_url)
    # æ·»åŠ ä»£ç†å‰ç¼€
    proxy_urls = []
    for base_url in candidate_urls:
        for proxy in PROXY_PREFIXES:
            if not base_url.startswith(proxy):
                proxy_url = proxy + base_url
                if proxy_url not in proxy_urls:
                    proxy_urls.append(proxy_url)
    # å»é‡å¹¶é™åˆ¶æ•°é‡
    unique_urls = list(dict.fromkeys(candidate_urls + proxy_urls))
    return unique_urls[:5]

def fetch_url_with_retry(url: str, timeout: int = 15) -> Optional[str]:
    """å¸¦é‡è¯•çš„URLæŠ“å–ï¼ˆè‡ªåŠ¨ä¿®å¤GitHub URLï¼‰"""
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    # è‡ªåŠ¨ä¿®å¤GitHub blob URL
    original_url = url
    if "github.com" in url and "/blob/" in url:
        url = url.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
        logger.info(f"è‡ªåŠ¨ä¿®å¤GitHub URLï¼š{original_url} â†’ {url}")
    # è·å–å€™é€‰URLåˆ—è¡¨
    candidate_urls = replace_github_domain(url)
    # åˆ†çº§è¶…æ—¶
    timeouts = [5, 10, 15, 15, 15]
    for idx, candidate in enumerate(candidate_urls):
        current_timeout = timeouts[min(idx, len(timeouts)-1)]
        try:
            logger.debug(f"å°è¯•æŠ“å– [{idx+1}/{len(candidate_urls)}]: {candidate} (è¶…æ—¶ï¼š{current_timeout}s)")
            response = requests.get(
                candidate,
                headers=headers,
                timeout=current_timeout,
                verify=False,
                allow_redirects=True
            )
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            logger.info(f"æˆåŠŸæŠ“å–ï¼š{candidate}")
            return response.text
        except requests.RequestException as e:
            logger.warning(f"æŠ“å–å¤±è´¥ [{idx+1}/{len(candidate_urls)}]: {candidate} | åŸå› ï¼š{str(e)[:50]}")
            continue
    logger.error(f"æ‰€æœ‰å€™é€‰é“¾æ¥éƒ½æŠ“å–å¤±è´¥ï¼š{original_url}")
    return None

def extract_m3u_meta(content: str, source_url: str) -> Tuple[OrderedDict, List[ChannelMeta]]:
    """
    M3Uç²¾å‡†æå–ï¼ˆæ ‡å‡†åŒ–group-titleå’Œé¢‘é“åï¼‰
    :return: (æŒ‰æ ‡å‡†åŒ–group-titleåˆ†ç±»çš„é¢‘é“å­—å…¸, å®Œæ•´çš„ChannelMetaåˆ—è¡¨)
    """
    m3u_pattern = re.compile(
        r"(#EXTINF:-?\d+.*?)\n\s*([^#\n\r\s].*?)(?=\s|#|$)",
        re.IGNORECASE | re.DOTALL | re.MULTILINE
    )
    attr_pattern = re.compile(r'(\w+)-(\w+)="([^"]*)"')
    categorized_channels = OrderedDict()
    meta_list = []
    seen_urls = set()
    matches = m3u_pattern.findall(content)
    logger.info(f"ä»M3Uå†…å®¹ä¸­åŒ¹é…åˆ° {len(matches)} ä¸ªåŸå§‹æ¡ç›®")
    
    for raw_extinf, url in matches:
        url = url.strip()
        raw_extinf = raw_extinf.strip()
        # è·³è¿‡æ— æ•ˆURLæˆ–é‡å¤URL
        if not url or not url.startswith(("http://", "https://")) or url in seen_urls:
            continue
        seen_urls.add(url)
        url_source_mapping[url] = source_url
        
        # è§£æ#EXTINFå±æ€§
        tvg_id, tvg_name, tvg_logo, group_title = None, None, None, None
        channel_name = "æœªçŸ¥é¢‘é“"
        attr_matches = attr_pattern.findall(raw_extinf)
        for attr1, attr2, value in attr_matches:
            if attr1 == "tvg" and attr2 == "id":
                tvg_id = value
            elif attr1 == "tvg" and attr2 == "name":
                tvg_name = standardize_cctv_name(value)  # æ ‡å‡†åŒ–tvg-name
            elif attr1 == "tvg" and attr2 == "logo":
                tvg_logo = value
            elif attr1 == "group" and attr2 == "title":
                group_title = clean_group_title(value)  # æ ¸å¿ƒä¿®æ”¹ï¼šæ ‡å‡†åŒ–group-title
        
        # æå–å¹¶æ ‡å‡†åŒ–é€—å·åçš„é¢‘é“å
        name_match = re.search(r',\s*(.+?)\s*$', raw_extinf)
        if name_match:
            channel_name = name_match.group(1).strip()
            channel_name = standardize_cctv_name(channel_name)
        
        # æœ€ç»ˆæ ‡å‡†åŒ–group-titleï¼ˆå…œåº•ï¼‰
        group_title = clean_group_title(group_title)
        
        # åˆ›å»ºå…ƒä¿¡æ¯å¯¹è±¡ï¼ˆå­˜å‚¨æ ‡å‡†åŒ–åçš„åç§°å’Œåˆ†ç±»ï¼‰
        meta = ChannelMeta(
            url=url,
            raw_extinf=raw_extinf,
            tvg_id=tvg_id,
            tvg_name=tvg_name,
            tvg_logo=tvg_logo,
            group_title=group_title,
            channel_name=channel_name,
            source_url=source_url
        )
        meta_list.append(meta)
        channel_meta_cache[url] = meta
        
        # æ·»åŠ åˆ°åˆ†ç±»å­—å…¸ï¼ˆåŸºäºæ ‡å‡†åŒ–åçš„group-titleï¼‰
        if group_title not in categorized_channels:
            categorized_channels[group_title] = []
        categorized_channels[group_title].append((channel_name, url))
    
    logger.info(f"M3Uç²¾å‡†æå–å®Œæˆï¼š{len(meta_list)}ä¸ªæœ‰æ•ˆé¢‘é“")
    logger.info(f"è¯†åˆ«çš„æ ‡å‡†åŒ–åˆ†ç±»ï¼š{list(categorized_channels.keys())}")
    return categorized_channels, meta_list

def extract_channels_from_content(content: str, source_url: str) -> OrderedDict:
    """
    æ™ºèƒ½æå–é¢‘é“ï¼ˆæ ‡å‡†åŒ–group-titleå’Œé¢‘é“åï¼‰
    å…ˆå…¨å±€æ›¿æ¢æºå†…å®¹ä¸­çš„ä¸è§„èŒƒåç§°ï¼Œå†è§£æ
    :return: æŒ‰æ ‡å‡†åŒ–åˆ†ç±»æ•´ç†çš„é¢‘é“å­—å…¸
    """
    categorized_channels = OrderedDict()
    # ä¼˜å…ˆå¤„ç†M3Uæ ¼å¼
    if "#EXTM3U" in content:
        m3u_categorized, _ = extract_m3u_meta(content, source_url)
        categorized_channels = m3u_categorized
    else:
        # æ™ºèƒ½è¯†åˆ«æ™®é€šæ–‡æœ¬ä¸­çš„é¢‘é“å’Œåˆ†ç±»
        lines = content.split('\n')
        current_group = "é»˜è®¤åˆ†ç±»"
        seen_urls = set()
        for line in lines:
            line = line.strip()
            if not line or line.startswith(("//", "#", "/*", "*/")):
                # è¯†åˆ«åˆ†ç±»è¡Œå¹¶æ ‡å‡†åŒ–
                if any(keyword in line.lower() for keyword in ['#åˆ†ç±»', '#genre', 'åˆ†ç±»:', 'genre:', '==', '---']):
                    group_match = re.search(r'[ï¼š:=](\S+)', line)
                    if group_match:
                        current_group = group_match.group(1).strip()
                    else:
                        current_group = re.sub(r'[#åˆ†ç±»:genre:==\-â€”]', '', line).strip() or "é»˜è®¤åˆ†ç±»"
                    # æ ¸å¿ƒä¿®æ”¹ï¼šæ ‡å‡†åŒ–æ™ºèƒ½è¯†åˆ«çš„åˆ†ç±»åï¼ˆå«æ˜ å°„åŒ¹é…ï¼‰
                    current_group = clean_group_title(current_group)
                    logger.debug(f"æ™ºèƒ½è¯†åˆ«å¹¶æ ‡å‡†åŒ–åˆ†ç±»ï¼š{current_group}")
                    continue
            # åŒ¹é…é¢‘é“å,URLæ ¼å¼
            pattern = r'([^,|#$]+)[,|#$]\s*(https?://[^\s,|#$]+)'
            matches = re.findall(pattern, line, re.IGNORECASE)
            if matches:
                for name, url in matches:
                    name = name.strip()
                    url = url.strip()
                    if not url or url in seen_urls:
                        continue
                    # æ ‡å‡†åŒ–åç§°ï¼ˆå…œåº•ï¼‰
                    standard_name = standardize_cctv_name(name)
                    seen_urls.add(url)
                    url_source_mapping[url] = source_url
                    
                    # æ™ºèƒ½åˆ†ç±»æ¨æ–­ï¼ˆåŸºäºæ ‡å‡†åŒ–åç§°ï¼‰+ æ ‡å‡†åŒ–åˆ†ç±»å
                    group_title = current_group
                    if any(keyword in standard_name for keyword in ['CCTV', 'å¤®è§†', 'ä¸­å¤®']):
                        group_title = "å¤®è§†é¢‘é“"  # å›ºå®šåˆ†ç±»åï¼Œå·²æ ‡å‡†åŒ–
                    elif any(keyword in standard_name for keyword in ['å«è§†', 'æ±Ÿè‹', 'æµ™æ±Ÿ', 'æ¹–å—', 'ä¸œæ–¹']):
                        group_title = "å«è§†é¢‘é“"  # å›ºå®šåˆ†ç±»åï¼Œå·²æ ‡å‡†åŒ–
                    elif any(keyword in standard_name for keyword in ['ç”µå½±', 'å½±è§†']):
                        group_title = "ç”µå½±é¢‘é“"  # å›ºå®šåˆ†ç±»åï¼Œå·²æ ‡å‡†åŒ–
                    elif any(keyword in standard_name for keyword in ['ä½“è‚²', 'CCTV5']):
                        group_title = "ä½“è‚²é¢‘é“"  # å›ºå®šåˆ†ç±»åï¼Œå·²æ ‡å‡†åŒ–
                    # æœ€ç»ˆæ ‡å‡†åŒ–åˆ†ç±»åï¼ˆå…œåº•ï¼Œå«æ˜ å°„åŒ¹é…ï¼‰
                    group_title = clean_group_title(group_title)
                    
                    # åˆ›å»ºå…ƒä¿¡æ¯
                    raw_extinf = f"#EXTINF:-1 tvg-id=\"\" tvg-name=\"{standard_name}\" tvg-logo=\"\" group-title=\"{group_title}\",{standard_name}"
                    meta = ChannelMeta(
                        url=url,
                        raw_extinf=raw_extinf,
                        tvg_id="",
                        tvg_name=standard_name,
                        tvg_logo="",
                        group_title=group_title,
                        channel_name=standard_name,
                        source_url=source_url
                    )
                    channel_meta_cache[url] = meta
                    
                    # æ·»åŠ åˆ°åˆ†ç±»å­—å…¸ï¼ˆåŸºäºæ ‡å‡†åŒ–åçš„group-titleï¼‰
                    if group_title not in categorized_channels:
                        categorized_channels[group_title] = []
                    categorized_channels[group_title].append((standard_name, url))
        
        logger.info(f"æ™ºèƒ½è¯†åˆ«å®Œæˆï¼š{sum(len(v) for v in categorized_channels.values())}ä¸ªæœ‰æ•ˆé¢‘é“")
        logger.info(f"è¯†åˆ«çš„æ ‡å‡†åŒ–åˆ†ç±»ï¼š{list(categorized_channels.keys())}")
    
    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªåˆ†ç±»
    if not categorized_channels:
        categorized_channels["æœªåˆ†ç±»"] = []
    return categorized_channels

def merge_channels(target: OrderedDict, source: OrderedDict):
    """åˆå¹¶é¢‘é“ï¼ˆURLå»é‡ï¼ŒåŸºäºæ ‡å‡†åŒ–åˆ†ç±»ï¼‰"""
    url_set = set()
    # æ”¶é›†å·²æœ‰çš„URL
    for category_name, ch_list in target.items():
        for _, url in ch_list:
            url_set.add(url)
    # åˆå¹¶æºæ•°æ®ï¼ˆåªæ·»åŠ æ–°URLï¼Œåˆ†ç±»åå·²æ ‡å‡†åŒ–ï¼Œå¯ç›´æ¥åˆå¹¶ï¼‰
    for category_name, channel_list in source.items():
        if category_name not in target:
            target[category_name] = []
        for name, url in channel_list:
            if url not in url_set:
                target[category_name].append((name, url))
                url_set.add(url)

def generate_summary(all_channels: OrderedDict):
    """ç”Ÿæˆæ±‡æ€»æ–‡ä»¶ï¼ˆä¿®å¤æ­£åˆ™è½¬ä¹‰é—®é¢˜ï¼Œå…¼å®¹ç‰¹æ®Šå­—ç¬¦ï¼‰"""
    summary_path = OUTPUT_FOLDER / "live_source_summary.txt"
    m3u_path = OUTPUT_FOLDER / "live_source_merged.m3u"
    try:
        # ç”Ÿæˆæ±‡æ€»TXT
        with open(summary_path, "w", encoding="utf-8") as f:
            f.write("IPTVç›´æ’­æºæ±‡æ€»ï¼ˆURLå»é‡+é¢‘é“åæ ‡å‡†åŒ–+åˆ†ç±»åæ ‡å‡†åŒ–ï¼‰\n")
            f.write("="*80 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»é¢‘é“æ•°ï¼š{sum(len(ch_list) for _, ch_list in all_channels.items())}\n")
            f.write(f"åˆ†ç±»æ•°ï¼š{len(all_channels)}\n")
            f.write("="*80 + "\n\n")
            # æŒ‰åˆ†ç±»å†™å…¥ï¼ˆåˆ†ç±»åå·²æ ‡å‡†åŒ–ï¼‰
            for group_title, channel_list in all_channels.items():
                f.write(f"ã€{group_title}ã€‘ï¼ˆ{len(channel_list)}ä¸ªé¢‘é“ï¼‰\n")
                for idx, (name, url) in enumerate(channel_list, 1):
                    source = url_source_mapping.get(url, "æœªçŸ¥æ¥æº")
                    f.write(f"{idx:>3}. {name:<20} {url}\n")
                    f.write(f"      æ¥æºï¼š{source}\n")
                f.write("\n")
        
        # ç”Ÿæˆåˆå¹¶åçš„M3Uæ–‡ä»¶ï¼ˆä¿®å¤æ­£åˆ™è½¬ä¹‰é—®é¢˜ï¼‰
        with open(m3u_path, "w", encoding="utf-8") as f:
            f.write("#EXTM3U x-tvg-url=\"\"\n")
            f.write(f"# IPTVç›´æ’­æºåˆå¹¶æ–‡ä»¶ï¼ˆURLå»é‡+é¢‘é“åæ ‡å‡†åŒ–+åˆ†ç±»åæ ‡å‡†åŒ–ï¼‰\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# æ€»é¢‘é“æ•°ï¼š{sum(len(ch_list) for _, ch_list in all_channels.items())}\n\n")
            
            for group_title, channel_list in all_channels.items():
                f.write(f"# ===== {group_title}ï¼ˆ{len(channel_list)}ä¸ªé¢‘é“ï¼‰ =====\n")
                for name, url in channel_list:
                    meta = channel_meta_cache.get(url)
                    if meta and meta.raw_extinf:
                        # æ›¿æ¢group-titleï¼ˆå­—ç¬¦ä¸²æ“ä½œï¼Œé¿å…æ­£åˆ™è½¬ä¹‰ï¼‰
                        standard_extinf = meta.raw_extinf
                        if 'group-title="' in standard_extinf:
                            start_idx = standard_extinf.find('group-title="') + len('group-title="')
                            end_idx = standard_extinf.find('"', start_idx)
                            if end_idx > start_idx:
                                standard_extinf = standard_extinf[:start_idx] + group_title + standard_extinf[end_idx:]
                        
                        # æ›¿æ¢é¢‘é“åï¼ˆå­—ç¬¦ä¸²åˆ†å‰²+æ‹¼æ¥ï¼Œé¿å…æ­£åˆ™è½¬ä¹‰ï¼‰
                        if ',' in standard_extinf:
                            extinf_part, old_name = standard_extinf.rsplit(',', 1)
                            # è½¬ä¹‰é¢‘é“åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                            safe_name = name.replace('\\', '\\\\').replace('$', '\\$')
                            standard_extinf = extinf_part + ',' + safe_name
                        f.write(standard_extinf + "\n")
                    else:
                        # ç›´æ¥ç”Ÿæˆæ ‡å‡†åŒ–çš„EXTINFè¡Œ
                        f.write(f"#EXTINF:-1 tvg-name=\"{name}\" group-title=\"{group_title}\",{name}\n")
                    f.write(url + "\n\n")
        
        logger.info(f"\næ±‡æ€»æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼š")
        logger.info(f"  - æ±‡æ€»ä¿¡æ¯ï¼š{summary_path}")
        logger.info(f"  - åˆå¹¶M3Uï¼š{m3u_path}")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæ±‡æ€»æ–‡ä»¶å¤±è´¥ï¼š{str(e)}", exc_info=True)

# ===================== ä¸»ç¨‹åº =====================
def main():
    """ä¸»å‡½æ•°ï¼šæŠ“å–â†’å…¨å±€æ›¿æ¢åç§°â†’æå–ï¼ˆæ ‡å‡†åŒ–åˆ†ç±»+é¢‘é“åï¼‰â†’å»é‡â†’æ±‡æ€»ç›´æ’­æº"""
    try:
        # æ¸…ç©ºç¼“å­˜
        global channel_meta_cache, url_source_mapping
        channel_meta_cache = {}
        url_source_mapping = {}
        logger.info("===== å¼€å§‹å¤„ç†ç›´æ’­æºï¼ˆå…¨é‡æ ‡å‡†åŒ–ç‰ˆï¼‰ =====")
        
        # ä»config.pyè·å–æºURLåˆ—è¡¨
        source_urls = getattr(config, 'SOURCE_URLS', [])
        if not source_urls:
            logger.error("config.pyä¸­æœªé…ç½®SOURCE_URLSï¼Œç¨‹åºç»ˆæ­¢")
            return
        logger.info(f"ä»é…ç½®ä¸­è¯»å–åˆ° {len(source_urls)} ä¸ªæºURL")
        
        # åˆå§‹åŒ–æ€»é¢‘é“å­—å…¸
        all_channels = OrderedDict()
        failed_urls = []
        
        # éå†æ‰€æœ‰æºURLï¼šæŠ“å–â†’å…¨å±€æ›¿æ¢â†’æå–ï¼ˆæ ‡å‡†åŒ–ï¼‰â†’åˆå¹¶
        for idx, url in enumerate(source_urls, 1):
            logger.info(f"\n===== å¤„ç†ç¬¬ {idx}/{len(source_urls)} ä¸ªæºï¼š{url} =====")
            # 1. æŠ“å–æºå†…å®¹
            content = fetch_url_with_retry(url)
            if content is None:
                failed_urls.append(url)
                continue
            # 2. å…¨å±€æ›¿æ¢æºå†…å®¹ä¸­çš„ä¸è§„èŒƒå¤®è§†é¢‘é“å
            content = global_replace_cctv_name(content)
            # 3. æå–é¢‘é“ï¼ˆåŒæ—¶æ ‡å‡†åŒ–group-titleå’Œé¢‘é“åï¼‰
            extracted_channels = extract_channels_from_content(content, url)
            # 4. åˆå¹¶é¢‘é“ï¼ˆè‡ªåŠ¨å»é‡ï¼Œåˆ†ç±»å·²æ ‡å‡†åŒ–ï¼‰
            merge_channels(all_channels, extracted_channels)
        
        # ç»Ÿè®¡ç»“æœ
        total_channels = sum(len(ch_list) for _, ch_list in all_channels.items())
        logger.info(f"\n===== å¤„ç†å®Œæˆç»Ÿè®¡ =====")
        logger.info(f"  - æºURLæ€»æ•°ï¼š{len(source_urls)}")
        logger.info(f"  - å¤±è´¥æºæ•°ï¼š{len(failed_urls)}")
        logger.info(f"  - å»é‡åæ€»é¢‘é“æ•°ï¼š{total_channels}")
        logger.info(f"  - æ ‡å‡†åŒ–åˆ†ç±»æ•°ï¼š{len(all_channels)}")
        logger.info(f"  - æ ‡å‡†åŒ–åˆ†ç±»åˆ—è¡¨ï¼š{list(all_channels.keys())}")
        if failed_urls:
            logger.info(f"  - å¤±è´¥çš„æºï¼š{', '.join(failed_urls)}")
        
        # ç”Ÿæˆæ±‡æ€»æ–‡ä»¶
        if total_channels > 0:
            generate_summary(all_channels)
        logger.info("\n===== æ‰€æœ‰æ“ä½œå®Œæˆ =====")
    except Exception as e:
        logger.critical(f"ç¨‹åºæ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}", exc_info=True)

if __name__ == "__main__":
    main()
