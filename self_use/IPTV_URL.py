# main.py
import re
import requests
import difflib
from urllib.parse import unquote, urlparse, urlunparse
import config

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´ï¼Œé¿å…è¢«åçˆ¬
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/plain, */*; q=0.01",
    "Accept-Encoding": "gzip, deflate",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
}

class M3UMerger:
    def __init__(self):
        self.similarity_threshold = config.SIMILARITY_THRESHOLD
        # æ ¸å¿ƒå­˜å‚¨ï¼škey=æ ‡å‡†åŒ–åçš„URLï¼Œvalue=æ•´åˆåçš„é¢‘é“ä¿¡æ¯
        self.channel_dict = {}

    def _replace_github_domain(self, url, new_domain):
        """æ›¿æ¢GitHub RAWé“¾æ¥çš„åŸŸå"""
        parsed = urlparse(url)
        if parsed.netloc in config.GITHUB_MIRRORS:
            new_parsed = parsed._replace(netloc=new_domain)
            return urlunparse(new_parsed)
        return url

    def _add_github_proxy(self, url, proxy_prefix):
        """ç»™GitHub RAWé“¾æ¥æ·»åŠ ä»£ç†å‰ç¼€"""
        parsed = urlparse(url)
        if parsed.netloc in config.GITHUB_MIRRORS:
            return proxy_prefix + url
        return url

    def download_m3u(self, original_url):
        """
        ä¸‹è½½M3Uå†…å®¹ï¼Œæ”¯æŒGitHubé•œåƒ/ä»£ç†è‡ªåŠ¨é‡è¯•
        :param original_url: åŸå§‹ç›´æ’­æºURL
        :return: M3Uå†…å®¹å­—ç¬¦ä¸² | None
        """
        # æ„å»ºå¾…å°è¯•çš„URLåˆ—è¡¨
        urls_to_try = [original_url]
        
        # 1. ç”Ÿæˆé•œåƒåŸŸåçš„URLï¼ˆå¦‚æœæ˜¯GitHub RAWé“¾æ¥ï¼‰
        if any(mirror in original_url for mirror in config.GITHUB_MIRRORS):
            for mirror in config.GITHUB_MIRRORS[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªï¼ˆåŸå§‹åŸŸåï¼‰
                urls_to_try.append(self._replace_github_domain(original_url, mirror))
        
        # 2. ç”Ÿæˆå¸¦ä»£ç†å‰ç¼€çš„URL
        for proxy in config.PROXY_PREFIXES:
            urls_to_try.append(self._add_github_proxy(original_url, proxy))
        
        # å»é‡ï¼ˆé¿å…é‡å¤å°è¯•ç›¸åŒURLï¼‰
        urls_to_try = list(dict.fromkeys(urls_to_try))
        
        # ä¾æ¬¡å°è¯•æ¯ä¸ªURL
        for idx, url in enumerate(urls_to_try):
            try:
                resp = requests.get(
                    url, 
                    headers=HEADERS, 
                    timeout=config.REQUEST_TIMEOUT,
                    allow_redirects=True  # å…è®¸é‡å®šå‘
                )
                resp.raise_for_status()  # æŠ›å‡ºHTTPé”™è¯¯ï¼ˆ4xx/5xxï¼‰
                # è‡ªåŠ¨è¯†åˆ«ç¼–ç ï¼Œé¿å…ä¹±ç 
                resp.encoding = resp.apparent_encoding if not resp.encoding else resp.encoding
                content = resp.text
                
                # æ£€æµ‹æ˜¯å¦ä¸ºM3Uå†…å®¹
                if "#EXTINF" not in content and "#EXTM3U" not in content:
                    if idx < len(urls_to_try)-1:
                        print(f"âš ï¸ {url} ä¸‹è½½çš„å†…å®¹éM3Uæ ¼å¼ï¼Œå°è¯•ä¸‹ä¸€ä¸ªé“¾æ¥...")
                        continue
                    else:
                        print(f"âŒ æ‰€æœ‰é“¾æ¥å‡æœªä¸‹è½½åˆ°æœ‰æ•ˆM3Uå†…å®¹ï¼š{original_url}")
                        return None
                
                print(f"âœ… æˆåŠŸä¸‹è½½ï¼ˆå°è¯•{idx+1}æ¬¡ï¼‰ï¼š{url}")
                return content
            except requests.exceptions.RequestException as e:
                if idx < len(urls_to_try)-1:
                    print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼ˆå°è¯•{idx+1}æ¬¡ï¼‰{url}: {str(e)[:30]}ï¼Œé‡è¯•ä¸‹ä¸€ä¸ª...")
                else:
                    print(f"âŒ æ‰€æœ‰é“¾æ¥å‡ä¸‹è½½å¤±è´¥ï¼š{original_url}ï¼Œé”™è¯¯ï¼š{str(e)[:50]}")
                    return None

    def parse_extinf_tags(self, m3u_content):
        """
        è§£æM3Uå†…å®¹ï¼Œæå–æ‰€æœ‰EXTINFæ ‡ç­¾å’Œå¯¹åº”URL
        è¿”å›æ ¼å¼ï¼š[{"tvg-id": "", "tvg-name": "", "tvg-logo": "", "group-title": "", "url": ""}, ...]
        """
        # åŒ¹é…EXTINFè¡Œ + ä¸‹ä¸€è¡Œçš„URLï¼ˆå…¼å®¹å„ç§ç©ºæ ¼/æ¢è¡Œæ ¼å¼ï¼‰
        pattern = re.compile(
            r'#EXTINF:-1\s*'
            r'(?:tvg-id="([^"]*)"\s*)?'       # å¯é€‰çš„tvg-id
            r'(?:tvg-name="([^"]*)"\s*)?'     # å¯é€‰çš„tvg-name
            r'(?:tvg-logo="([^"]*)"\s*)?'     # å¯é€‰çš„tvg-logo
            r'(?:group-title="([^"]*)"\s*)?'  # å¯é€‰çš„group-title
            r'.*?\n'                          # è¡Œå°¾å‰©ä½™å†…å®¹
            r'([^\r\n]+)',                    # é¢‘é“URLï¼ˆéç©ºè¡Œï¼‰
            re.IGNORECASE | re.MULTILINE
        )
        
        channels = []
        matches = pattern.findall(m3u_content)
        for tvg_id, tvg_name, tvg_logo, group_title, url in matches:
            # æ ‡å‡†åŒ–å¤„ç†ï¼šå»ç©ºæ ¼ã€è§£ç URLã€ç»Ÿä¸€ç©ºå€¼ä¸º""
            clean_url = unquote(url.strip())  # è§£ç URLä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚%20ï¼‰
            channel = {
                "tvg-id": tvg_id.strip() or "",
                "tvg-name": tvg_name.strip() or "",
                "tvg-logo": tvg_logo.strip() or "",
                "group-title": group_title.strip() or "",
                "url": clean_url
            }
            # è¿‡æ»¤æ— æ•ˆURL
            if channel["url"] and not channel["url"].startswith("#"):
                channels.append(channel)
        
        # è¾“å‡ºè§£æè¯¦æƒ…
        if len(channels) == 0:
            print("âš ï¸ æœªæå–åˆ°ä»»ä½•EXTINFä¿¡æ¯ï¼ˆå¯èƒ½æºæ–‡ä»¶æ ¼å¼ä¸è§„èŒƒï¼‰")
        else:
            print(f"   è§£æå‡º {len(channels)} ä¸ªåŸå§‹é¢‘é“")
        return channels

    def calculate_similarity(self, chan1, chan2):
        """è®¡ç®—ä¸¤ä¸ªé¢‘é“å­—æ®µçš„ç›¸ä¼¼åº¦ï¼ˆä»…ç”¨äºURLä¸åŒæ—¶çš„è¿‘ä¼¼åˆ¤æ–­ï¼‰"""
        # ä¼˜å…ˆç”¨tvg-idç²¾ç¡®åŒ¹é…
        if chan1["tvg-id"] and chan2["tvg-id"] and chan1["tvg-id"] == chan2["tvg-id"]:
            return 1.0
        
        # è®¡ç®—tvg-nameç›¸ä¼¼åº¦ï¼ˆæ ¸å¿ƒå­—æ®µï¼‰
        name_sim = difflib.SequenceMatcher(None, chan1["tvg-name"], chan2["tvg-name"]).ratio()
        # è®¡ç®—group-titleç›¸ä¼¼åº¦ï¼ˆè¾…åŠ©å­—æ®µï¼‰
        group_sim = difflib.SequenceMatcher(None, chan1["group-title"], chan2["group-title"]).ratio()
        
        # åŠ æƒå¹³å‡ï¼štvg-nameå 70%ï¼Œgroup-titleå 30%
        return (name_sim * 0.7) + (group_sim * 0.3)

    def merge_channel(self, new_channel):
        """
        åˆå¹¶é¢‘é“ï¼š
        1. URLç›¸åŒ â†’ ä¿ç•™ä¿¡æ¯æ›´å®Œæ•´çš„EXTINFæ ‡ç­¾
        2. URLä¸åŒ â†’ å­—æ®µç›¸ä¼¼åº¦â‰¥é˜ˆå€¼æ‰åˆ¤å®šä¸ºé‡å¤ï¼Œå¦åˆ™ä¿ç•™
        """
        url = new_channel["url"]
        
        # 1. URLå»é‡ä¼˜å…ˆï¼šæ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
        if url in self.channel_dict:
            existing = self.channel_dict[url]
            # æ•´åˆä¿¡æ¯ï¼šä¿ç•™éç©ºå­—æ®µï¼ˆæ–°é¢‘é“æœ‰å€¼åˆ™è¦†ç›–æ—§çš„ç©ºå€¼ï¼‰
            self.channel_dict[url] = {
                "tvg-id": existing["tvg-id"] or new_channel["tvg-id"],
                "tvg-name": existing["tvg-name"] or new_channel["tvg-name"],
                "tvg-logo": existing["tvg-logo"] or new_channel["tvg-logo"],
                "group-title": existing["group-title"] or new_channel["group-title"],
                "url": url
            }
            return
        
        # 2. URLä¸åŒæ—¶ï¼Œæ£€æŸ¥å­—æ®µè¿‘ä¼¼åº¦ï¼ˆé¿å…é‡å¤é¢‘é“ï¼‰
        for existing_url, existing_chan in self.channel_dict.items():
            sim_score = self.calculate_similarity(new_channel, existing_chan)
            if sim_score >= self.similarity_threshold:
                # è¿‘ä¼¼åŒ¹é…ï¼šä¿ç•™ä¿¡æ¯æ›´å®Œæ•´çš„é‚£ä¸ª
                if self.count_non_empty_fields(new_channel) > self.count_non_empty_fields(existing_chan):
                    self.channel_dict[existing_url] = new_channel
                return
        
        # 3. æ— é‡å¤ï¼Œæ–°å¢é¢‘é“
        self.channel_dict[url] = new_channel

    def count_non_empty_fields(self, channel):
        """ç»Ÿè®¡é¢‘é“éç©ºå­—æ®µæ•°é‡ï¼ˆç”¨äºåˆ¤æ–­ä¿¡æ¯å®Œæ•´æ€§ï¼‰"""
        return sum(1 for v in channel.values() if v and v != channel["url"])

    def generate_m3u_file(self):
        """ç”Ÿæˆæœ€ç»ˆçš„M3Uæ–‡ä»¶ï¼ŒæŒ‰group-titleåˆ†ç»„æ’åº"""
        # æŒ‰group-titleåˆ†ç»„
        grouped_channels = {}
        for channel in self.channel_dict.values():
            group = channel["group-title"] or "æœªåˆ†ç»„"
            if group not in grouped_channels:
                grouped_channels[group] = []
            grouped_channels[group].append(channel)
        
        # å†™å…¥M3Uæ–‡ä»¶
        try:
            with open(config.OUTPUT_FILE, "w", encoding="utf-8") as f:
                # M3Uæ ‡å‡†å¤´éƒ¨
                f.write("#EXTM3U x-tvg-url=\"https://epg.112114.xyz/pp.xml\"\n\n")
                
                # æŒ‰åˆ†ç»„åç§°æ’åºï¼Œé€ä¸ªå†™å…¥
                for group in sorted(grouped_channels.keys()):
                    channels = sorted(grouped_channels[group], key=lambda x: x["tvg-name"].lower())
                    for chan in channels:
                        # æ„å»ºEXTINFè¡Œï¼ˆåªä¿ç•™éç©ºå­—æ®µï¼‰
                        extinf_parts = ["#EXTINF:-1"]
                        if chan["tvg-id"]:
                            extinf_parts.append(f'tvg-id="{chan["tvg-id"]}"')
                        if chan["tvg-name"]:
                            extinf_parts.append(f'tvg-name="{chan["tvg-name"]}"')
                        if chan["tvg-logo"]:
                            extinf_parts.append(f'tvg-logo="{chan["tvg-logo"]}"')
                        if chan["group-title"]:
                            extinf_parts.append(f'group-title="{chan["group-title"]}"')
                    
                    # å†™å…¥ä¸€è¡ŒEXTINF + ä¸€è¡ŒURL
                    f.write(" ".join(extinf_parts) + "\n")
                    f.write(chan["url"] + "\n\n")
            
            print(f"\nâœ… ç”ŸæˆæˆåŠŸï¼æ–‡ä»¶è·¯å¾„ï¼š{config.OUTPUT_FILE}")
            print(f"ğŸ“Š ç»Ÿè®¡ï¼šåŸå§‹å»é‡åä¿ç•™ {len(self.channel_dict)} ä¸ªæœ‰æ•ˆé¢‘é“")
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥ï¼š{e}")

    def run(self):
        """ä¸»æ‰§è¡Œæµç¨‹"""
        print("ğŸš€ å¼€å§‹å¤„ç†ç›´æ’­æº...")
        total_parsed = 0
        
        # éå†æ‰€æœ‰ç›´æ’­æºURL
        for idx, url in enumerate(config.LIVE_SOURCE_URLS, 1):
            print(f"\n[{idx}/{len(config.LIVE_SOURCE_URLS)}] å¤„ç†ï¼š{url}")
            # ä¸‹è½½M3Uå†…å®¹ï¼ˆè‡ªåŠ¨é‡è¯•é•œåƒ/ä»£ç†ï¼‰
            m3u_content = self.download_m3u(url)
            if not m3u_content:
                continue
            
            # è§£æEXTINFæ ‡ç­¾
            channels = self.parse_extinf_tags(m3u_content)
            total_parsed += len(channels)
            
            # é€ä¸ªåˆå¹¶ï¼ˆå»é‡+æ•´åˆä¿¡æ¯ï¼‰
            for chan in channels:
                self.merge_channel(chan)
        
        # ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶
        self.generate_m3u_file()
        print(f"\nğŸ“ˆ æ€»è§£æé¢‘é“æ•°ï¼š{total_parsed} | å»é‡åä¿ç•™ï¼š{len(self.channel_dict)}")

if __name__ == "__main__":
    # å®ä¾‹åŒ–å¹¶è¿è¡Œ
    merger = M3UMerger()
    merger.run()
